{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§  Unit 5.2: Deep Reinforcement Learning (DQN)\n",
                "\n",
                "**Course:** Advanced Machine Learning (AICC 303)  \n",
                "**Topic:** 5.6 Deep Reinforcement Learning (Deep Q-Network)\n",
                "\n",
                "**Why Deep RL?**\n",
                "Tabular Q-Learning works for small state spaces (like grids). But what about a self-driving car? The state space (camera pixels) is infinite.\n",
                "We use a **Neural Network** to approximate the Q-Function: $Q(s, a; \\theta) \\approx Q^*(s, a)$.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import gymnasium as gym\n",
                "import random\n",
                "from collections import deque\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "\n",
                "# Setup\n",
                "env = gym.make('CartPole-v1')\n",
                "state_size = env.observation_space.shape[0]\n",
                "action_size = env.action_space.n\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "print(\"State Size:\", state_size)  # [Pos, Vel, Angle, AngVel]\n",
                "print(\"Action Size:\", action_size)  # Left, Right"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The DQN Agent\n",
                "\n",
                "Key components:\n",
                "1.  **Model**: NN that takes State â†’ Predicts Q-values for all Actions.\n",
                "2.  **Memory (Replay Buffer)**: Stores $(s, a, r, s', done)$ tuples. We train on random batches from here to break correlations.\n",
                "3.  **Epsilon-Greedy**: Policy for exploration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class QNetwork(nn.Module):\n",
                "    def __init__(self, state_size, action_size):\n",
                "        super(QNetwork, self).__init__()\n",
                "        self.fc1 = nn.Linear(state_size, 24)\n",
                "        self.fc2 = nn.Linear(24, 24)\n",
                "        self.fc3 = nn.Linear(24, action_size)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = F.relu(self.fc1(x))\n",
                "        x = F.relu(self.fc2(x))\n",
                "        return self.fc3(x)\n",
                "\n",
                "class DQNAgent:\n",
                "    def __init__(self, state_size, action_size):\n",
                "        self.state_size = state_size\n",
                "        self.action_size = action_size\n",
                "        self.memory = deque(maxlen=2000)\n",
                "        self.gamma = 0.95\n",
                "        self.epsilon = 1.0\n",
                "        self.epsilon_min = 0.01\n",
                "        self.epsilon_decay = 0.995\n",
                "        self.learning_rate = 0.001\n",
                "        self.model = QNetwork(state_size, action_size).to(device)\n",
                "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
                "        self.criterion = nn.MSELoss()\n",
                "\n",
                "    def remember(self, state, action, reward, next_state, done):\n",
                "        self.memory.append((state, action, reward, next_state, done))\n",
                "\n",
                "    def act(self, state):\n",
                "        if np.random.rand() <= self.epsilon:\n",
                "            return random.randrange(self.action_size)\n",
                "        \n",
                "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
                "        self.model.eval()\n",
                "        with torch.no_grad():\n",
                "            act_values = self.model(state)\n",
                "        self.model.train()\n",
                "        return torch.argmax(act_values[0]).item()\n",
                "\n",
                "    def replay(self, batch_size):\n",
                "        if len(self.memory) < batch_size:\n",
                "            return\n",
                "            \n",
                "        minibatch = random.sample(self.memory, batch_size)\n",
                "        \n",
                "        states = torch.FloatTensor([t[0] for t in minibatch]).to(device)\n",
                "        actions = torch.LongTensor([t[1] for t in minibatch]).to(device)\n",
                "        rewards = torch.FloatTensor([t[2] for t in minibatch]).to(device)\n",
                "        next_states = torch.FloatTensor([t[3] for t in minibatch]).to(device)\n",
                "        dones = torch.FloatTensor([t[4] for t in minibatch]).to(device)\n",
                "        \n",
                "        # Current Q values\n",
                "        # gather picks the Q-value for the specific action taken\n",
                "        current_q = self.model(states).gather(1, actions.unsqueeze(1))\n",
                "        \n",
                "        # Target Q values\n",
                "        with torch.no_grad():\n",
                "            max_next_q = self.model(next_states).max(1)[0]\n",
                "            target_q = rewards + (1 - dones) * self.gamma * max_next_q\n",
                "        \n",
                "        # Compute loss\n",
                "        loss = self.criterion(current_q.squeeze(), target_q)\n",
                "        \n",
                "        # Optimize\n",
                "        self.optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        self.optimizer.step()\n",
                "        \n",
                "        # Decay epsilon\n",
                "        if self.epsilon > self.epsilon_min:\n",
                "            self.epsilon *= self.epsilon_decay"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Training Loop\n",
                "We train the agent to balance a pole on a moving cart (`CartPole-v1`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "agent = DQNAgent(state_size, action_size)\n",
                "EPISODES = 50  # Kept low for demo speed. Real training needs ~1000\n",
                "\n",
                "for e in range(EPISODES):\n",
                "    state, _ = env.reset()\n",
                "    \n",
                "    for time in range(500):\n",
                "        action = agent.act(state)\n",
                "        \n",
                "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
                "        done = terminated or truncated\n",
                "        \n",
                "        reward = reward if not done else -10  # Penalize dropping the pole\n",
                "        \n",
                "        agent.remember(state, action, reward, next_state, done)\n",
                "        state = next_state\n",
                "        \n",
                "        if done:\n",
                "            print(f\"episode: {e}/{EPISODES}, score: {time}, e: {agent.epsilon:.2f}\")\n",
                "            break\n",
                "    \n",
                "    # Train model after episode\n",
                "    agent.replay(32)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}