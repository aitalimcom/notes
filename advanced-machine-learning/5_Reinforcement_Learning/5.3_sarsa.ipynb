{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§  Unit 5.3: SARSA (State-Action-Reward-State-Action)\n",
                "\n",
                "**Course:** Advanced Machine Learning (AICC 303)  \n",
                "**Topic:** 5.4 Model Free Algorithms (SARSA)\n",
                "\n",
                "**Difference from Q-Learning:**\n",
                "*   **Q-Learning (Off-Policy):** Learns the value of the *optimal* policy, even if acting randomly.\n",
                "*   **SARSA (On-Policy):** Learns the value of the *policy being followed* (including exploration).\n",
                "\n",
                "**Update Rule:**\n",
                "$Q(s,a) \\leftarrow Q(s,a) + \\alpha [R + \\gamma Q(s',a') - Q(s,a)]$\n",
                "(Notice we use the actual next action $a'$, not $\\max_{a'} Q$.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import gymnasium as gym\n",
                "\n",
                "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
                "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
                "\n",
                "alpha = 0.8\n",
                "gamma = 0.95\n",
                "epsilon = 0.1\n",
                "episodes = 1000\n",
                "\n",
                "def choose_action(state):\n",
                "    if np.random.rand() < epsilon:\n",
                "        return env.action_space.sample()\n",
                "    return np.argmax(q_table[state, :])\n",
                "\n",
                "for i in range(episodes):\n",
                "    state, _ = env.reset()\n",
                "    action = choose_action(state)\n",
                "    \n",
                "    done = False\n",
                "    trunc = False\n",
                "    \n",
                "    while not (done or trunc):\n",
                "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
                "        done = terminated or truncated\n",
                "        \n",
                "        next_action = choose_action(next_state)\n",
                "        \n",
                "        # SARSA Update\n",
                "        q_target = reward + gamma * q_table[next_state, next_action]\n",
                "        q_table[state, action] += alpha * (q_target - q_table[state, action])\n",
                "        \n",
                "        state = next_state\n",
                "        action = next_action\n",
                "\n",
                "print(\"Final Q-Table (SARSA):\\n\", np.round(q_table, 2))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}