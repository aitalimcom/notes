{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§  Unit 5.5: Policy Gradient Methods (REINFORCE)\n",
                "\n",
                "**Course:** Advanced Machine Learning (AICC 303)  \n",
                "**Topic:** 5.5 Policy Gradient Methods (REINFORCE)\n",
                "\n",
                "So far (Q-Learning, SARSA, DQN), we used **Value-Based** methods: estimate $Q(s,a)$ and then pick $\\text{argmax} Q$.\n",
                "**Policy-Based** methods learn the probability distribution of actions $\\pi(a|s; \\theta)$ directly.\n",
                "\n",
                "**Advantages:**\n",
                "1.  Can handle continuous action spaces.\n",
                "2.  Can learn stochastic policies (e.g., in Rock-Paper-Scissors).\n",
                "\n",
                "**REINFORCE Algorithm (Monte Carlo Policy Gradient):**\n",
                "1.  Run an entire episode.\n",
                "2.  Calculate the total return $G_t$.\n",
                "3.  Increase probability of actions that resulted in high $G_t$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting gymnasium\n",
                        "  Downloading gymnasium-1.2.3-py3-none-any.whl.metadata (10 kB)\n",
                        "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\fulbutte\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (2.1.2)\n",
                        "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\python312\\lib\\site-packages (from gymnasium) (3.1.2)\n",
                        "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\fulbutte\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (4.12.2)\n",
                        "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
                        "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
                        "Downloading gymnasium-1.2.3-py3-none-any.whl (952 kB)\n",
                        "   ---------------------------------------- 0.0/952.1 kB ? eta -:--:--\n",
                        "   ----------- ---------------------------- 262.1/952.1 kB ? eta -:--:--\n",
                        "   ---------------------- ----------------- 524.3/952.1 kB 1.1 MB/s eta 0:00:01\n",
                        "   --------------------------------- ------ 786.4/952.1 kB 1.8 MB/s eta 0:00:01\n",
                        "   --------------------------------- ------ 786.4/952.1 kB 1.8 MB/s eta 0:00:01\n",
                        "   ---------------------------------------- 952.1/952.1 kB 959.6 kB/s  0:00:00\n",
                        "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
                        "Installing collected packages: farama-notifications, gymnasium\n",
                        "\n",
                        "   ---------------------------------------- 0/2 [farama-notifications]\n",
                        "   ---------------------------------------- 0/2 [farama-notifications]\n",
                        "   ---------------------------------------- 0/2 [farama-notifications]\n",
                        "   ---------------------------------------- 0/2 [farama-notifications]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   -------------------- ------------------- 1/2 [gymnasium]\n",
                        "   ---------------------------------------- 2/2 [gymnasium]\n",
                        "\n",
                        "Successfully installed farama-notifications-0.0.4 gymnasium-1.2.3\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
                        "\n",
                        "[notice] A new release of pip is available: 25.2 -> 25.3\n",
                        "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
                    ]
                }
            ],
            "source": [
                "!pip install gymnasium"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gymnasium as gym\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "from torch.distributions import Categorical\n",
                "\n",
                "env = gym.make('CartPole-v1')\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "class PolicyNetwork(nn.Module):\n",
                "    def __init__(self, state_size, action_size):\n",
                "        super(PolicyNetwork, self).__init__()\n",
                "        self.fc1 = nn.Linear(state_size, 24)\n",
                "        self.fc2 = nn.Linear(24, 24)\n",
                "        self.fc3 = nn.Linear(24, action_size)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = F.relu(self.fc1(x))\n",
                "        x = F.relu(self.fc2(x))\n",
                "        return F.softmax(self.fc3(x), dim=-1)  # Outputs Probabilities\n",
                "\n",
                "model = PolicyNetwork(env.observation_space.shape[0], env.action_space.n).to(device)\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
                "\n",
                "def compute_returns(rewards, gamma=0.99):\n",
                "    \"\"\"Compute discounted returns\"\"\"\n",
                "    returns = []\n",
                "    G =0\n",
                "    for r in reversed(rewards):\n",
                "        G = r + gamma * G\n",
                "        returns.insert(0, G)\n",
                "    \n",
                "    # Normalize (Baseline reduction for variance)\n",
                "    returns = torch.tensor(returns)\n",
                "    returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
                "    return returns"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Episode 0, Total Reward: 21.0, Loss: -0.0970\n",
                        "Episode 10, Total Reward: 15.0, Loss: 0.1465\n",
                        "Episode 20, Total Reward: 15.0, Loss: 0.1363\n",
                        "Episode 30, Total Reward: 20.0, Loss: -0.4313\n",
                        "Episode 40, Total Reward: 40.0, Loss: -0.4784\n",
                        "Episode 50, Total Reward: 24.0, Loss: -0.2114\n",
                        "Episode 60, Total Reward: 47.0, Loss: -1.4042\n",
                        "Episode 70, Total Reward: 63.0, Loss: 0.1203\n",
                        "Episode 80, Total Reward: 29.0, Loss: -0.2223\n",
                        "Episode 90, Total Reward: 27.0, Loss: 0.3090\n",
                        "Episode 100, Total Reward: 30.0, Loss: -0.2519\n",
                        "Episode 110, Total Reward: 48.0, Loss: 0.1980\n",
                        "Episode 120, Total Reward: 59.0, Loss: -1.9546\n",
                        "Episode 130, Total Reward: 57.0, Loss: -1.2323\n",
                        "Episode 140, Total Reward: 39.0, Loss: -0.4346\n",
                        "Episode 150, Total Reward: 72.0, Loss: -2.2292\n",
                        "Episode 160, Total Reward: 26.0, Loss: 1.1136\n",
                        "Episode 170, Total Reward: 57.0, Loss: -1.5818\n",
                        "Episode 180, Total Reward: 59.0, Loss: -0.4601\n",
                        "Episode 190, Total Reward: 35.0, Loss: 0.8967\n"
                    ]
                }
            ],
            "source": [
                "episodes = 200  # Need more to converge usually\n",
                "\n",
                "for episode in range(episodes):\n",
                "    state, _ = env.reset()\n",
                "    \n",
                "    log_probs = []\n",
                "    rewards = []\n",
                "    done = False\n",
                "    trunc = False\n",
                "    \n",
                "    while not (done or trunc):\n",
                "        # Convert state to tensor\n",
                "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
                "        \n",
                "        # Get Action Probabilities\n",
                "        probs = model(state_tensor)\n",
                "        \n",
                "        # Sample action from distribution\n",
                "        m = Categorical(probs)\n",
                "        action = m.sample()\n",
                "        \n",
                "        next_state, reward, done, trunc, _ = env.step(action.item())\n",
                "        \n",
                "        log_probs.append(m.log_prob(action))\n",
                "        rewards.append(reward)\n",
                "        \n",
                "        state = next_state\n",
                "    \n",
                "    # Compute returns\n",
                "    returns = compute_returns(rewards)\n",
                "    \n",
                "    # Compute loss\n",
                "    policy_loss = []\n",
                "    for log_prob, G in zip(log_probs, returns):\n",
                "        policy_loss.append(-log_prob * G)\n",
                "    \n",
                "    # Update Policy after full episode (Monte Carlo)\n",
                "    optimizer.zero_grad()\n",
                "    loss = torch.stack(policy_loss).sum()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    \n",
                "    if episode % 10 == 0:\n",
                "        print(f\"Episode {episode}, Total Reward: {sum(rewards)}, Loss: {loss.item():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
