{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§  Unit 5.1: Reinforcement Learning Fundamentals\n",
                "\n",
                "**Course:** Advanced Machine Learning (AICC 303)  \n",
                "**Topic:** 5.1 RL Fundamentals, MDP, Q-Learning (Tabular)\n",
                "\n",
                "**Goal:** Understand how agents learn from interaction with an environment.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import gymnasium as gym\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Create Environment (FrozenLake-v1)\n",
                "# is_slippery=False makes it deterministic for easier learning\n",
                "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode=None)\n",
                "\n",
                "print(\"Action Space:\", env.action_space)  # Discrete(4): Left, Down, Right, Up\n",
                "print(\"State Space:\", env.observation_space) # Discrete(16): 4x4 Grid"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Q-Learning (Tabular)\n",
                "\n",
                "**Bellman Equation:**\n",
                "$Q(s,a) \\leftarrow Q(s,a) + \\alpha [R + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$\n",
                "\n",
                "*   $\\alpha$: Learning Rate\n",
                "*   $\\gamma$: Discount Factor\n",
                "*   $\\epsilon$: Exploration Rate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Q-Table with zeros\n",
                "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
                "\n",
                "# Hyperparameters\n",
                "alpha = 0.8\n",
                "gamma = 0.95\n",
                "epsilon = 0.1\n",
                "episodes = 1000\n",
                "\n",
                "rewards_all = []\n",
                "\n",
                "for i in range(episodes):\n",
                "    state, _ = env.reset()\n",
                "    done = False\n",
                "    trunc = False\n",
                "    total_reward = 0\n",
                "    \n",
                "    while not (done or trunc):\n",
                "        # Exploration vs Exploitation\n",
                "        if np.random.rand() < epsilon:\n",
                "            action = env.action_space.sample()\n",
                "        else:\n",
                "            action = np.argmax(q_table[state, :])\n",
                "        \n",
                "        # Take Action\n",
                "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
                "        done = terminated or truncated\n",
                "        \n",
                "        # Update Q-Table\n",
                "        q_table[state, action] = q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state, :]) - q_table[state, action])\n",
                "        \n",
                "        state = next_state\n",
                "        total_reward += reward\n",
                "        \n",
                "    rewards_all.append(total_reward)\n",
                "\n",
                "print(\"Training Finished\")\n",
                "print(f\"Success Rate: {sum(rewards_all)/episodes:.2f}\")\n",
                "print(\"\\nFinal Q-Table Values:\\n\", np.round(q_table, 2))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}