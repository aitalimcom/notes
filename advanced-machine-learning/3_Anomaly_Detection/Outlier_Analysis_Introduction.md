# Introduction to Outlier Analysis
*Based on "Outlier Analysis" by Charu C. Aggarwal and Scikit-Learn Documentation*

## 1. Definition and Nature of Outliers
An **outlier** is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism (Hawkins, 1980).
*   **Synonyms:** Anomalies, discordants, deviants, abnormalities.
*   **Key Applications:** Intrusion detection, Credit-card fraud, Medical diagnosis, Earth science (sensor data), Law enforcement.

### 1.1 Noise vs. Anomalies
There is a spectrum from **Normal Data** $\rightarrow$ **Noise** $\rightarrow$ **Anomalies**.
*   **Noise:** Weak outliers, often not interesting to the analyst but distinct from the "strong" patterns.
*   **Anomalies:** Strong deviations of interest (e.g., fraud).
*   **Interpretation:** The distinction is often subjective and application-dependent. "One person's noise is another person's signal."

## 2. The Data Model
The choice of model is crucial. An incorrect model (e.g., assuming Gaussian distribution for Zipfian data) leads to poor detection.
*   **Z-Value Test:** $Z_i = |X_i - \mu| / \sigma$. Effective for normal distributions. High Z-values ($>3$) indicate outliers.
*   **Supervised vs. Unsupervised:**
    *   *Unsupervised:* No labels. deviations from the "normal" model are outliers.
    *   *Supervised:* Labels available. Treated as rare class classification or imbalanced learning.

---

## 3. Taxonomy of Outlier Detection Models

### 3.1 Extreme-Value Analysis
*   Focuses on statistical tails of distributions.
*   Good for 1D data or aggregating scores from other models.
*   *Limitation:* Does not find outliers in central sparse regions (e.g., violations of linear dependencies).

### 3.2 Probabilistic and Statistical Models
*   Assume data follows a specific distribution (e.g., Gaussian Mixture Models).
*   Parameters learned via Expectation-Maximization (EM).
*   *Score:* Inverse of probability density (or log-likelihood).

### 3.3 Linear Models
*   Model data into lower-dimensional subspaces (PCA, Regression).
*   *Idea:* Outliers have large **reconstruction errors** (distance from the hyperplane).
*   *Application:* effective for noise correction and dimensional reduction.

### 3.4 Proximity-Based Models
*   **Clustering:** Outliers are points far from cluster centroids or in small clusters.
*   **Density-Based:** Measure local density (e.g., LOF). Low density = Outlier.
*   **Nearest-Neighbor:** Distance to $k^{th}$ nearest neighbor. Computationally expensive ($O(N^2)$) but effective.

### 3.5 Information-Theoretic Models
*   Based on Minimum Description Length (MDL).
*   *Idea:* Outliers increase the code length required to describe the data. Removal of outliers "compresses" the data description significantly.

### 3.6 High-Dimensional Detection (Subspace Analysis)
*   *Curse of Dimensionality:* In high dimensions, data becomes sparse, and distances concentrate (everything looks equidistant).
*   *Solution:* **Subspace Outlier Detection**. Find outliers in specific lower-dimensional projections where they stand out.

---

## 4. Outlier Ensembles
Combining multiple algorithms to improve robustness.
1.  **Sequential Ensembles:** Algorithm $A_{t+1}$ depends on results of $A_t$ (e.g., removing obvious outliers first to refine the model).
2.  **Independent Ensembles:** Run multiple algorithms (or independent runs of randomized algorithms) and combine scores (e.g., Feature Bagging, Isolation Forest).

---

## 5. Practical Implementation: Isolation Forest (Scikit-Learn)
*Algorithm based on randomized partitioning.*

### 5.1 Core Concept
*   **Isolation:** Anomalies are "few and different". They are easier to isolate (separate) than normal points.
*   **Mechanism:** Randomly select a feature and a split value. Construct a tree.
*   **Metric:** **Path Length**. Anomalies appear much closer to the root (shorter path) than normal points (deep in the tree).

### 5.2 Scikit-Learn API (`sklearn.ensemble.IsolationForest`)
*   **`n_estimators`**: Number of trees (default 100).
*   **`contamination`**: Expected proportion of outliers (default 'auto'). Used to set the threshold for binary decision.
*   **`max_samples`**: Number of samples to draw for each tree (subsampling prevents swamping and masking).

**key Methods:**
*   `fit(X)`: Build the forest.
*   `decision_function(X)`: Average anomaly score. (Lower is more abnormal, but Sklearn convention: negative scores = outliers, positive = inliers).
*   `predict(X)`: Returns -1 for outliers and 1 for inliers.

### 5.3 Code Snippet
```python
from sklearn.ensemble import IsolationForest
import numpy as np

# Data: Normal cluster + outliers
X = np.vstack([np.random.normal(0, 1, (100, 2)), [[5, 5], [-3, 3]]])

# Initialize and Fit
clf = IsolationForest(contamination=0.02, random_state=42)
clf.fit(X)

# Predict (-1 is outlier, 1 is inlier)
preds = clf.predict(X)
scores = clf.decision_function(X) # < 0 typically outlier
```
