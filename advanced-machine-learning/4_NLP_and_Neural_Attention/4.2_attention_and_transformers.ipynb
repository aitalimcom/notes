{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ¤– Unit 4.2: Attention & Transformers\n",
                "\n",
                "**Course:** Advanced Machine Learning (AICC 303)  \n",
                "**Topics:**\n",
                "*   4.6 Attention Mechanisms\n",
                "*   4.7 Types of Attention\n",
                "*   4.8 Transformer\n",
                "\n",
                "**The Revolution:** In 2017, the paper \"Attention is All You Need\" changed NLP forever by removing Recurrence (RNNs) and relying entirely on Attention mechanisms.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Setup\n",
                "sns.set(style=\"whitegrid\")\n",
                "torch.manual_seed(42)\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Attention Mechanism\n",
                "\n",
                "In Seq2Seq (Encoder-Decoder), the Encoder had to compress the entire sentence into a *single fixed-size vector*. This caused information loss for long sentences.\n",
                "\n",
                "**Attention** allows the Decoder to \"look back\" at all Encoder hidden states and focus on relevant words for the current prediction.\n",
                "\n",
                "### 1.1 Self-Attention (The Core of Transformers)\n",
                "Every word in the sentence looks at every other word to understand context.\n",
                "\n",
                "**Formula:**\n",
                "$$ Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
                "\n",
                "*   **Q (Query):** What am I looking for?\n",
                "*   **K (Key):** What do I have to offer?\n",
                "*   **V (Value):** What is the actual content?\n",
                "\n",
                "If $Q$ matches $K$ (high dot product), we take more of $V$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Output Shape: torch.Size([1, 3, 4])\n",
                        "Attention Weights:\n",
                        " [[[0.3017341  0.30983964 0.3884262 ]\n",
                        "  [0.2450955  0.3801395  0.37476498]\n",
                        "  [0.29378855 0.2293217  0.4768897 ]]]\n"
                    ]
                }
            ],
            "source": [
                "def scaled_dot_product_attention(q, k, v, mask=None):\n",
                "    \"\"\"\n",
                "    Calculate the attention weights.\n",
                "    q, k, v must have matching leading dimensions.\n",
                "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
                "    \"\"\"\n",
                "    # matmul_qk = torch.bmm(q, k.transpose(1, 2))\n",
                "    matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
                "\n",
                "    # Scale matmul_qk\n",
                "    dk = k.size(-1)\n",
                "    scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
                "\n",
                "    # Add mask if present (for decoder to not look ahead)\n",
                "    if mask is not None:\n",
                "        scaled_attention_logits += (mask * -1e9)\n",
                "\n",
                "    # Softmax to get probabilities\n",
                "    attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
                "\n",
                "    output = torch.matmul(attention_weights, v)\n",
                "\n",
                "    return output, attention_weights\n",
                "\n",
                "# Example Usage\n",
                "torch.manual_seed(42)\n",
                "temp_q = torch.randn(1, 3, 4)  # (Batch, Seq_Len, Dim)\n",
                "temp_k = torch.randn(1, 3, 4)\n",
                "temp_v = torch.randn(1, 3, 4)\n",
                "\n",
                "output, weights = scaled_dot_product_attention(temp_q, temp_k, temp_v)\n",
                "print(\"Output Shape:\", output.shape)\n",
                "print(\"Attention Weights:\\n\", weights.numpy())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Using Transformers (Hugging Face)\n",
                "\n",
                "Instead of training from scratch (which requires massive data), we use Pre-trained models like BERT or GPT.\n",
                "\n",
                "**Install needed libraries:** `pip install transformers`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
                        "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "11b925d50df2436f88a239c8d3951d2b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\fulbutte\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\fulbutte\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
                        "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
                        "  warnings.warn(message)\n",
                        "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4d5d925fcff44ace913055b33126f295",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "cb793587d06b4c39ba9183e1090d44bd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f9808f08014c409c84358af65dbac767",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.txt: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sentiment: [{'label': 'POSITIVE', 'score': 0.999861478805542}]\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "24dc706a4f6e4c36a5dfa872d58754a4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\fulbutte\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\fulbutte\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
                        "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
                        "  warnings.warn(message)\n",
                        "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "db2952c56f334d91b57467b56a404a99",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
                        "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "155a1c4ea94f4588ac3080398842a585",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ee9402b498664de1811eb580979f30b4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "83cc17f5047744878757b8968b8c93dd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "BERT Predictions for [MASK]:\n",
                        "technology: 0.3353\n",
                        "science: 0.2559\n",
                        "reality: 0.0211\n"
                    ]
                }
            ],
            "source": [
                "# !pip install transformers\n",
                "from transformers import pipeline\n",
                "\n",
                "# 1. Sentiment Analysis (using a DistilBERT model by default)\n",
                "classifier = pipeline(\"sentiment-analysis\")\n",
                "result = classifier(\"I absolutely loved the advanced machine learning course!\")\n",
                "print(f\"Sentiment: {result}\")\n",
                "\n",
                "# 2. Masked Language Modeling (BERT)\n",
                "# BERT is trained to predict missing words.\n",
                "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
                "result_mask = unmasker(\"Artificial Intelligence is the [MASK] of the future.\")\n",
                "\n",
                "print(\"\\nBERT Predictions for [MASK]:\")\n",
                "for r in result_mask[:3]:\n",
                "    print(f\"{r['token_str']}: {r['score']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Tokenizer Visualization\n",
                "Transformers use special tokenizers (Subword tokenization)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "05eb2d4a59b045e192a8f82616464598",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\fulbutte\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\fulbutte\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
                        "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
                        "  warnings.warn(message)\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "bd8f7e54c344459da386f31671afcecf",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "2237142569da4ff6bdb6918fcf93e543",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9cea89e937734432b96a0217c355bba3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original: Transformer models are powerful.\n",
                        "Token IDs: [101, 13809, 23763, 3584, 1132, 3110, 119, 102]\n",
                        "Decoded Tokens: ['[CLS]', 'Trans', '##former', 'models', 'are', 'powerful', '.', '[SEP]']\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoTokenizer\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
                "text = \"Transformer models are powerful.\"\n",
                "encoded_input = tokenizer(text)\n",
                "\n",
                "print(\"Original:\", text)\n",
                "print(\"Token IDs:\", encoded_input['input_ids'])\n",
                "print(\"Decoded Tokens:\", tokenizer.convert_ids_to_tokens(encoded_input['input_ids']))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
