{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ—£ï¸ Unit 4.1: NLP Fundamentals\n",
                "\n",
                "**Course:** Advanced Machine Learning (AICC 303)  \n",
                "**Topics:**\n",
                "*   4.2 Text Tokenization and Normalization\n",
                "*   4.3 Lemmatization and Stemming\n",
                "*   4.4 Text Embedding (Bag of Words, TF-IDF, Word2Vec)\n",
                "\n",
                "**Goal:** Computers don't understand text. We must convert text into numbers (vectors) before feeding it to a model.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: nltk in c:\\python312\\lib\\site-packages (3.9.2)\n",
                        "Requirement already satisfied: click in c:\\python312\\lib\\site-packages (from nltk) (8.3.0)\n",
                        "Requirement already satisfied: joblib in c:\\python312\\lib\\site-packages (from nltk) (1.5.2)\n",
                        "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\fulbutte\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2025.9.18)\n",
                        "Requirement already satisfied: tqdm in c:\\users\\fulbutte\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.67.1)\n",
                        "Requirement already satisfied: colorama in c:\\users\\fulbutte\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
                        "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
                        "\n",
                        "[notice] A new release of pip is available: 25.2 -> 25.3\n",
                        "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
                    ]
                }
            ],
            "source": [
                "!pip install nltk"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\fulbutte\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt_tab to\n",
                        "[nltk_data]     C:\\Users\\fulbutte\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     C:\\Users\\fulbutte\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n",
                        "[nltk_data] Downloading package wordnet to\n",
                        "[nltk_data]     C:\\Users\\fulbutte\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n",
                        "[nltk_data] Downloading package omw-1.4 to\n",
                        "[nltk_data]     C:\\Users\\fulbutte\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import nltk\n",
                "import re\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
                "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
                "\n",
                "# Download necessary NLTK data\n",
                "nltk.download('punkt')\n",
                "nltk.download('punkt_tab')\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')\n",
                "nltk.download('omw-1.4')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Text Preprocessing\n",
                "\n",
                "Raw text is messy. We need to clean it.\n",
                "1.  **Lowercasing**: \"Apple\" == \"apple\"\n",
                "2.  **Tokenization**: Splitting sentences into words.\n",
                "3.  **Stopword Removal**: Removing common words like \"the\", \"is\", \"and\".\n",
                "4.  **Stemming/Lemmatization**: Converting \"running\" -> \"run\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens: ['the', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'the', 'lazy', 'dog', '.', 'it', 'was', 'amazing', '!']\n",
                        "Filtered: ['quick', 'brown', 'foxes', 'jumping', 'lazy', 'dog', 'amazing']\n",
                        "Stemmed: ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog', 'amaz']\n",
                        "Lemmatized: ['quick', 'brown', 'fox', 'jumping', 'lazy', 'dog', 'amazing']\n"
                    ]
                }
            ],
            "source": [
                "text = \"The quick brown foxes are jumping over the lazy dog. It was amazing!\"\n",
                "\n",
                "# 1. Tokenization\n",
                "tokens = nltk.word_tokenize(text.lower())\n",
                "print(f\"Tokens: {tokens}\")\n",
                "\n",
                "# 2. Stopword Removal\n",
                "stop_words = set(stopwords.words('english'))\n",
                "filtered_tokens = [w for w in tokens if w.isalnum() and w not in stop_words]\n",
                "print(f\"Filtered: {filtered_tokens}\")\n",
                "\n",
                "# 3. Stemming vs Lemmatization\n",
                "stemmer = PorterStemmer()\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "\n",
                "stemmed = [stemmer.stem(w) for w in filtered_tokens]\n",
                "lemmatized = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
                "\n",
                "print(f\"Stemmed: {stemmed}\")\n",
                "print(f\"Lemmatized: {lemmatized}\")\n",
                "\n",
                "# Note: Stemmers chop off ends (foxes->fox), Lemmatizers use dictionary (better but slower)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Text Representation (Vectorization)\n",
                "\n",
                "### 2.1 Bag of Words (BoW)\n",
                "Creates a matrix where rows are documents and columns are unique words. Value = Count."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Vocabulary: ['deep' 'great' 'is' 'learning' 'love' 'machine']\n",
                        "BoW Matrix:\n",
                        " [[0 0 0 1 1 1]\n",
                        " [0 1 1 1 0 1]\n",
                        " [1 0 0 1 1 0]]\n"
                    ]
                }
            ],
            "source": [
                "corpus = [\n",
                "    \"I love machine learning.\",\n",
                "    \"Machine learning is great.\",\n",
                "    \"I love deep learning.\"\n",
                "]\n",
                "\n",
                "vectorizer = CountVectorizer()\n",
                "X_bow = vectorizer.fit_transform(corpus)\n",
                "\n",
                "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
                "print(\"BoW Matrix:\\n\", X_bow.toarray())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 TF-IDF (Term Frequency - Inverse Document Frequency)\n",
                "Instead of counts, we weight words.\n",
                "*   **TF**: High if word appears often in THIS document.\n",
                "*   **IDF**: Low if word appears in MANY documents (like \"the\").\n",
                "\n",
                "Helps find \"unique\" keywords."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "TF-IDF Matrix:\n",
                        " [[0.   0.   0.   0.48 0.62 0.62]\n",
                        " [0.   0.58 0.58 0.35 0.   0.44]\n",
                        " [0.72 0.   0.   0.43 0.55 0.  ]]\n"
                    ]
                }
            ],
            "source": [
                "tfidf = TfidfVectorizer()\n",
                "X_tfidf = tfidf.fit_transform(corpus)\n",
                "\n",
                "print(\"TF-IDF Matrix:\\n\", np.round(X_tfidf.toarray(), 2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Word Embeddings (Deep Learning)\n",
                "\n",
                "BoW and TF-IDF are \"Sparse\" and high-dimensional. They don't capture meaning (e.g., \"King\" and \"Queen\" are unrelated).\n",
                "\n",
                "**Embeddings** map words to dense vectors (e.g., size 50 or 100) where similar words are close in space.\n",
                "\n",
                "### Word2Vec / GloVe\n",
                "Algorithms trained on massive text (Wikipedia) to learn these relationships.\n",
                "*   King - Man + Woman â‰ˆ Queen"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Word: 'machine'\n",
                        "Embedding (Vector of size 8):\n",
                        "[[-0.25702578  0.6040811   0.95967674  2.116546   -0.09270746 -0.6710528\n",
                        "   1.510489    1.5074047 ]]\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "# PyTorch Embedding Layer Example\n",
                "vocab_size = 50\n",
                "embedding_dim = 8\n",
                "\n",
                "# Simple tokenization (word to index)\n",
                "word_to_idx = {}\n",
                "for doc in corpus:\n",
                "    for word in doc.lower().split():\n",
                "        if word not in word_to_idx:\n",
                "            word_to_idx[word] = len(word_to_idx)\n",
                "\n",
                "# Create embedding layer\n",
                "embedding = nn.Embedding(len(word_to_idx), embedding_dim)\n",
                "\n",
                "# Example: get embedding for a word\n",
                "word = \"machine\"\n",
                "if word in word_to_idx:\n",
                "    word_idx = torch.LongTensor([word_to_idx[word]])\n",
                "    word_embedding = embedding(word_idx)\n",
                "    print(f\"\\nWord: '{word}'\")\n",
                "    print(f\"Embedding (Vector of size {embedding_dim}):\\n{word_embedding.detach().numpy()}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
